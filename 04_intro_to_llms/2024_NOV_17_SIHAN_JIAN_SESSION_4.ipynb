{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**HOMEWORK DESCRIPTION:**\n",
        "\n",
        "\n",
        "Load in a generative model using the HuggingFace pipeline and generate text using a batch of prompts.\n",
        "Play with generative parameters such as temperature, max_new_tokens, and the model itself and explain the effect on the legibility of the model response. Try at least 4 different parameter/model combinations.\n",
        "Models that can be used include:\n",
        "google/gemma-2-2b-it\n",
        "microsoft/Phi-3-mini-4k-instruct\n",
        "meta-llama/Llama-3.2-1B\n",
        "Any model from this list: Text-generation models\n",
        "gpt2 if having trouble loading these models in"
      ],
      "metadata": {
        "id": "jVsiqVRscPE5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoP-ONpacNPf"
      },
      "outputs": [],
      "source": [
        "# Install the required library if you haven't already\n",
        "# !pip install transformers\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the text generation pipeline with GPT-2 or other available models\n",
        "generator_gpt2 = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "generator_phi = pipeline(\"text-generation\", model=\"microsoft/Phi-3-mini-4k-instruct\")  # Alternative model\n",
        "\n",
        "# Define a batch of prompts\n",
        "prompts = [\n",
        "    \"Once upon a time in a distant galaxy,\",\n",
        "    \"The key to a successful life is\",\n",
        "    \"In the year 2050, humanity will\",\n",
        "    \"The secret to happiness lies\"\n",
        "]\n",
        "\n",
        "# Experiment with different parameter settings\n",
        "combinations = [\n",
        "    {\"generator\": generator_gpt2, \"temperature\": 0.7, \"max_new_tokens\": 50},\n",
        "    {\"generator\": generator_gpt2, \"temperature\": 0.2, \"max_new_tokens\": 100},\n",
        "    {\"generator\": generator_gpt2, \"temperature\": 0.5, \"max_new_tokens\": 30},\n",
        "    {\"generator\": generator_phi, \"temperature\": 1.0, \"max_new_tokens\": 75}  # Using a different model\n",
        "]\n",
        "\n",
        "# Generate outputs for each combination\n",
        "results = {}\n",
        "for i, params in enumerate(combinations):\n",
        "    generator = params.pop(\"generator\")\n",
        "    results[f\"Combination {i+1}\"] = [generator(prompt, **params)[0][\"generated_text\"] for prompt in prompts]\n",
        "\n",
        "# Print the results for comparison\n",
        "for combination, outputs in results.items():\n",
        "    print(f\"\\n{combination}:\")\n",
        "    for i, output in enumerate(outputs):\n",
        "        print(f\"Prompt {i+1}: {output}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HOMEWORK DESCRIPTION:**\n",
        "\n",
        "Load in 2 models of different parameter size (e.g. GPT2, meta-llama/Llama-2-7b-chat-hf, or distilbert/distilgpt2) and analyze the BertViz for each. How does the attention mechanisms change depending on model size?"
      ],
      "metadata": {
        "id": "wAnDPWVTnvZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers bertviz\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from bertviz import model_view\n",
        "\n",
        "# Load smaller model (e.g., GPT-2)\n",
        "tokenizer_gpt2 = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model_gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\", output_attentions=True))\n",
        "\n",
        "# Load larger model (e.g., LLaMA 2)\n",
        "tokenizer_llama = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
        "model_llama = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", output_attentions=True))\n",
        "\n",
        "# Define a sample input text\n",
        "text = \"The universe is vast and full of mysteries.\"\n",
        "\n",
        "# Tokenize input\n",
        "inputs_gpt2 = tokenizer_gpt2(text, return_tensors=\"pt\")\n",
        "inputs_llama = tokenizer_llama(text, return_tensors=\"pt\")\n",
        "\n",
        "# Generate outputs and visualize attention with BertViz for the smaller model\n",
        "print(\"Visualizing GPT-2 attention:\")\n",
        "outputs_gpt2 = model_gpt2(inputs_gpt2)\n",
        "attention_gpt2 = outputs_gpt2[-1]\n",
        "tokens_gpt2 = tokenizer_gpt2.convert_ids_to_tokens(inputs_gpt2[0])\n",
        "model_view(attention_gpt2, tokens_gpt2)\n",
        "\n",
        "# Generate outputs and visualize attention with BertViz for the larger model\n",
        "print(\"\\nVisualizing LLaMA-2 attention:\")\n",
        "outputs_llama = model_llama(inputs_llama)\n",
        "attention_llama = outputs_llama[-1]\n",
        "tokens_llama = tokenizer_llama.convert_ids_to_tokens(inputs_llama[0])\n",
        "model_view(attention_llama, tokens_llama)"
      ],
      "metadata": {
        "id": "N5xtb8kxl1aZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}